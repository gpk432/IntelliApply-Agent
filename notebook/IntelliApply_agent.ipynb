{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IntelliApply: Automated Job Matching & Application Preparation Agent\n",
        "\n",
        "This notebook implements an AI agent system that:\n",
        "- Ingests a user's resume (PDF, DOCX, or TXT)\n",
        "- Extracts their profile and job preferences using Gemini\n",
        "- Fetches relevant remote jobs from the web\n",
        "- Ranks and filters jobs by fit\n",
        "- Tailors resume content and generates cover letters\n",
        "- Exports tailored resume and cover letter as PDFs per job\n",
        "\n",
        "Built for the Kaggle + Google Agents Intensive Capstone (Enterprise Agents track)."
      ],
      "metadata": {
        "id": "-Lov_OZMwfT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "OzH3NLVEx-Bt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7eSTHwAqwUlB"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai pymupdf docx2txt fpdf2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports & Gemini Configuration"
      ],
      "metadata": {
        "id": "AaGWDpakxTS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import textwrap\n",
        "import re\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Optional\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import getpass\n",
        "import requests\n",
        "import fitz\n",
        "import docx2txt\n",
        "from fpdf import FPDF\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import files\n"
      ],
      "metadata": {
        "id": "x5uYacXiQLpd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Gemini client configuration\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def configure_gemini() -> None:\n",
        "    \"\"\"Configure the Gemini client using an API key stored in the environment.\"\"\"\n",
        "    if not os.environ.get(\"GEMINI_API_KEY\"):\n",
        "        os.environ[\"GEMINI_API_KEY\"] = getpass.getpass(\"Gemini API key: \")\n",
        "\n",
        "    api_key = os.environ[\"GEMINI_API_KEY\"]\n",
        "    if not api_key:\\\n",
        "        raise ValueError(\"GEMINI_API_KEY is empty.\")\n",
        "\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "\n",
        "MODEL_NAME = \"models/gemini-2.5-flash\"\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"intelliapply\")\n",
        "\n",
        "# simple in-memory session state\n",
        "USER_PROFILE_STORE = None\n",
        "APPLIED_JOBS_HISTORY: List[dict] = []\n",
        "\n",
        "configure_gemini()\n",
        "\n",
        "# check\n",
        "_model = genai.GenerativeModel(MODEL_NAME)\n",
        "print(_model.generate_content(\"IntelliApply system check: OK\").text.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "jmEUnXtFQXBb",
        "outputId": "0ad25042-a245-4ef0-f6fd-ea6ae75ebfbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API key: ··········\n",
            "Understood. IntelliApply system status is confirmed as OK.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data models"
      ],
      "metadata": {
        "id": "5y-DjDkcxUP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Data models\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class Preferences:\n",
        "    \"\"\"Job search preferences for a candidate.\"\"\"\n",
        "    target_titles: List[str]\n",
        "    locations: List[str]\n",
        "    remote_only: bool = False\n",
        "    min_salary: Optional[int] = None\n",
        "    preferred_companies: Optional[List[str]] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ExperienceItem:\n",
        "    title: str\n",
        "    company: str\n",
        "    start_date: Optional[str]  # \"YYYY-MM\" or None\n",
        "    end_date: Optional[str]\n",
        "    bullets: List[str]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ProjectItem:\n",
        "    name: str\n",
        "    description: str\n",
        "    bullets: List[str]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class UserProfile:\n",
        "    name: str\n",
        "    summary: str\n",
        "    skills: List[str]\n",
        "    experience: List[ExperienceItem]\n",
        "    projects: List[ProjectItem]\n",
        "    education: List[str]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class JobDescription:\n",
        "    job_id: str\n",
        "    title: str\n",
        "    company: str\n",
        "    location: Optional[str]\n",
        "    salary_estimate: Optional[str]\n",
        "    description: str\n",
        "    requirements: Optional[str]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MatchResult:\n",
        "    job: JobDescription\n",
        "    score: float\n",
        "    reason: str\n",
        "    selected: bool\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TailoredResult:\n",
        "    job_id: str\n",
        "    tailored_summary: str\n",
        "    ordered_skills: List[str]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ApplicationPackage:\n",
        "    job: JobDescription\n",
        "    match_score: float\n",
        "    match_reason: str\n",
        "    tailored_summary: str\n",
        "    ordered_skills: List[str]\n",
        "    cover_letter: str"
      ],
      "metadata": {
        "id": "IO8-xQtcxQK2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shared utilities"
      ],
      "metadata": {
        "id": "HmRMM7whxVaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Utility functions\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def call_gemini_json(prompt: str, system_instruction: str) -> dict:\n",
        "    \"\"\"\n",
        "    Call Gemini with a system instruction and prompt.\n",
        "    Expect a JSON-only response and return the parsed object.\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(\n",
        "        MODEL_NAME,\n",
        "        system_instruction=system_instruction,\n",
        "    )\n",
        "    response = model.generate_content(prompt)\n",
        "    text = response.text.strip()\n",
        "\n",
        "    if text.startswith(\"```\"):\n",
        "        # Handle fenced blocks such as ```json ... ```\n",
        "        text = text.strip(\"`\")\n",
        "        text = text.replace(\"json\", \"\", 1).strip()\n",
        "\n",
        "    return json.loads(text)\n",
        "\n",
        "\n",
        "def log_match(job_id: str, score: float, selected: bool, reason: str) -> None:\n",
        "    \"\"\"Log a single job match result.\"\"\"\n",
        "    logger.info(\n",
        "        \"match job_id=%s score=%.2f selected=%s reason=%s\",\n",
        "        job_id,\n",
        "        score,\n",
        "        selected,\n",
        "        reason,\n",
        "    )\n",
        "\n",
        "\n",
        "def remember_application(job: JobDescription, decision: str, score: float) -> None:\n",
        "    \"\"\"Record the user's decision on a job into the in-memory history.\"\"\"\n",
        "    APPLIED_JOBS_HISTORY.append(\n",
        "        {\n",
        "            \"job_id\": job.job_id,\n",
        "            \"title\": job.title,\n",
        "            \"company\": job.company,\n",
        "            \"decision\": decision,\n",
        "            \"score\": score,\n",
        "            # timezone-aware UTC timestamp (fixes utcnow deprecation)\n",
        "            \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "def strip_html(html: str) -> str:\n",
        "    \"\"\"Remove HTML tags and collapse whitespace.\"\"\"\n",
        "    text = re.sub(r\"<[^>]+>\", \" \", html or \"\")\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "\n",
        "def sanitize_pdf_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize text for FPDF core fonts:\n",
        "    - Replace em/en dashes with a simple hyphen\n",
        "    - Drop characters outside Latin-1\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    text = text.replace(\"—\", \"-\").replace(\"–\", \"-\")\n",
        "    # Encode/decode through latin-1 to strip unsupported characters\n",
        "    return text.encode(\"latin-1\", \"ignore\").decode(\"latin-1\")\n"
      ],
      "metadata": {
        "id": "NmKU0EOsQl9S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resume ingestion (PDF/DOCX/TXT --> text)"
      ],
      "metadata": {
        "id": "4qFAjiCtxW9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Resume ingestion\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def load_resume_text_from_upload() -> str:\n",
        "    \"\"\"\n",
        "    Upload a resume file (PDF, DOCX, or TXT) via Colab and return its text.\n",
        "    \"\"\"\n",
        "    print(\"Upload your resume file (PDF, DOCX, or TXT).\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        raise ValueError(\"No file uploaded.\")\n",
        "\n",
        "    filename = next(iter(uploaded.keys()))\n",
        "    content = uploaded[filename]\n",
        "    print(f\"Loaded file: {filename}\")\n",
        "\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        with fitz.open(stream=content, filetype=\"pdf\") as doc:\n",
        "            text = \"\".join(page.get_text() for page in doc)\n",
        "        return text\n",
        "\n",
        "    if filename.lower().endswith(\".docx\"):\n",
        "        tmp_path = \"/tmp/resume.docx\"\n",
        "        with open(tmp_path, \"wb\") as f:\n",
        "            f.write(content)\n",
        "        return docx2txt.process(tmp_path)\n",
        "\n",
        "    # Fallback: treat as plain text\n",
        "    return content.decode(\"utf-8\", errors=\"ignore\")"
      ],
      "metadata": {
        "id": "FprifLlhxQqu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Profile parsing agent (resume --> UserProfile)"
      ],
      "metadata": {
        "id": "i0cjs6l8xXsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Profile parsing\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def parse_resume_to_profile(resume_text: str) -> UserProfile:\n",
        "    \"\"\"\n",
        "    Convert free-form resume text into a structured UserProfile.\n",
        "    The model is instructed not to fabricate experience or skills.\n",
        "    \"\"\"\n",
        "    system_instruction = (\n",
        "        \"Extract a concise, structured profile from a resume. \"\n",
        "        \"Do not invent skills, companies, dates, or roles.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Return ONLY valid JSON with this structure:\n",
        "\n",
        "{{\n",
        "  \"name\": \"...\",\n",
        "  \"summary\": \"...\",\n",
        "  \"skills\": [\"skill1\", \"skill2\"],\n",
        "  \"experience\": [\n",
        "    {{\n",
        "      \"title\": \"...\",\n",
        "      \"company\": \"...\",\n",
        "      \"start_date\": \"YYYY-MM\" or null,\n",
        "      \"end_date\": \"YYYY-MM\" or null,\n",
        "      \"bullets\": [\"...\"]\n",
        "    }}\n",
        "  ],\n",
        "  \"projects\": [\n",
        "    {{\n",
        "      \"name\": \"...\",\n",
        "      \"description\": \"...\",\n",
        "      \"bullets\": [\"...\"]\n",
        "    }}\n",
        "  ],\n",
        "  \"education\": [\"...\"]\n",
        "}}\n",
        "\n",
        "RESUME:\n",
        "{resume_text}\n",
        "\"\"\"\n",
        "\n",
        "    data = call_gemini_json(prompt, system_instruction)\n",
        "\n",
        "    experience = [\n",
        "        ExperienceItem(\n",
        "            title=e[\"title\"],\n",
        "            company=e[\"company\"],\n",
        "            start_date=e.get(\"start_date\"),\n",
        "            end_date=e.get(\"end_date\"),\n",
        "            bullets=e.get(\"bullets\", []),\n",
        "        )\n",
        "        for e in data.get(\"experience\", [])\n",
        "    ]\n",
        "\n",
        "    projects = [\n",
        "        ProjectItem(\n",
        "            name=p[\"name\"],\n",
        "            description=p.get(\"description\", \"\"),\n",
        "            bullets=p.get(\"bullets\", []),\n",
        "        )\n",
        "        for p in data.get(\"projects\", [])\n",
        "    ]\n",
        "\n",
        "    profile = UserProfile(\n",
        "        name=data.get(\"name\", \"Candidate\"),\n",
        "        summary=data.get(\"summary\", \"\"),\n",
        "        skills=data.get(\"skills\", []),\n",
        "        experience=experience,\n",
        "        projects=projects,\n",
        "        education=data.get(\"education\", []),\n",
        "    )\n",
        "    return profile\n",
        "\n",
        "\n",
        "def setup_profile_and_prefs(resume_text: str, prefs: Preferences) -> UserProfile:\n",
        "    \"\"\"Create a UserProfile from resume text and store it in session state.\"\"\"\n",
        "    global USER_PROFILE_STORE\n",
        "    profile = parse_resume_to_profile(resume_text)\n",
        "    USER_PROFILE_STORE = profile\n",
        "    logger.info(\"profile name=%s skills=%d experience_items=%d\",\n",
        "                profile.name, len(profile.skills), len(profile.experience))\n",
        "    logger.info(\"preferences=%s\", prefs)\n",
        "    return profile"
      ],
      "metadata": {
        "id": "H8LinIPIxQyr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preference extraction agent (from resume)"
      ],
      "metadata": {
        "id": "wtXjoJY-xYUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Preference extraction\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def extract_preferences_from_resume_text(resume_text: str) -> Preferences:\n",
        "    \"\"\"\n",
        "    Infer job search preferences (titles, locations, remote, salary, companies)\n",
        "    from the resume text.\n",
        "    \"\"\"\n",
        "    system_instruction = (\n",
        "        \"Read a resume and infer realistic job search preferences for the candidate. \"\n",
        "        \"Return only JSON, no explanations.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Return ONLY JSON with this structure:\n",
        "\n",
        "{{\n",
        "  \"target_titles\": [\"Software Engineer Intern\", \"Machine Learning Engineer Intern\"],\n",
        "  \"locations\": [\"United States\", \"Remote\"],\n",
        "  \"remote_only\": false,\n",
        "  \"min_salary\": null,\n",
        "  \"preferred_companies\": [\"Google\", \"Microsoft\"]\n",
        "}}\n",
        "\n",
        "Rules:\n",
        "- target_titles: likely roles the candidate is pursuing.\n",
        "- locations: countries / regions / 'Remote' they are open to.\n",
        "- remote_only: true only if resume strongly prefers remote work.\n",
        "- min_salary: integer or null if unclear.\n",
        "- preferred_companies: names if the resume mentions targets; otherwise [].\n",
        "\n",
        "RESUME:\n",
        "{resume_text}\n",
        "\"\"\"\n",
        "\n",
        "    data = call_gemini_json(prompt, system_instruction)\n",
        "\n",
        "    prefs = Preferences(\n",
        "        target_titles=data.get(\"target_titles\", []),\n",
        "        locations=data.get(\"locations\", []),\n",
        "        remote_only=data.get(\"remote_only\", False),\n",
        "        min_salary=data.get(\"min_salary\"),\n",
        "        preferred_companies=data.get(\"preferred_companies\"),\n",
        "    )\n",
        "    logger.info(\"inferred preferences=%s\", prefs)\n",
        "    return prefs\n"
      ],
      "metadata": {
        "id": "XOE3N7LexQ5m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Job fetcher agent (Remotive API)"
      ],
      "metadata": {
        "id": "iTqp7wdqxY8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Job fetcher\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def fetch_jobs_from_remotive(search_term: str, limit: int = 10) -> List[JobDescription]:\n",
        "    \"\"\"\n",
        "    Fetch remote jobs matching a search term from the Remotive API.\n",
        "    \"\"\"\n",
        "    url = \"https://remotive.com/api/remote-jobs\"\n",
        "    params = {\"search\": search_term, \"limit\": limit}\n",
        "    response = requests.get(url, params=params, timeout=20)\n",
        "    response.raise_for_status()\n",
        "    data = response.json()\n",
        "    jobs_data = data.get(\"jobs\", [])\n",
        "\n",
        "    jobs: List[JobDescription] = []\n",
        "    for j in jobs_data:\n",
        "        description = strip_html(j.get(\"description\", \"\"))\n",
        "        jobs.append(\n",
        "            JobDescription(\n",
        "                job_id=str(j.get(\"id\")),\n",
        "                title=j.get(\"title\", \"\"),\n",
        "                company=j.get(\"company_name\", \"\"),\n",
        "                location=j.get(\"candidate_required_location\") or \"Remote\",\n",
        "                salary_estimate=j.get(\"salary\") or None,\n",
        "                description=description,\n",
        "                requirements=None,\n",
        "            )\n",
        "        )\n",
        "    return jobs\n",
        "\n",
        "\n",
        "def fetch_jobs_for_preferences(prefs: Preferences, per_title: int = 10) -> List[JobDescription]:\n",
        "    \"\"\"\n",
        "    Fetch jobs for each target title in preferences and de-duplicate by job_id.\n",
        "    \"\"\"\n",
        "    all_jobs: List[JobDescription] = []\n",
        "    seen_ids = set()\n",
        "\n",
        "    for title in prefs.target_titles:\n",
        "        for job in fetch_jobs_from_remotive(title, limit=per_title):\n",
        "            if job.job_id in seen_ids:\n",
        "                continue\n",
        "            seen_ids.add(job.job_id)\n",
        "            all_jobs.append(job)\n",
        "\n",
        "    logger.info(\"fetched_jobs=%d titles=%s\", len(all_jobs), prefs.target_titles)\n",
        "    return all_jobs"
      ],
      "metadata": {
        "id": "3VI_6HX7xRBI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matching and ranking agent"
      ],
      "metadata": {
        "id": "PtwCNoFGxZff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Matching and ranking\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def compute_match_score(profile: UserProfile, job: JobDescription, prefs: Preferences) -> MatchResult:\n",
        "    \"\"\"\n",
        "    Compute a simple fit score based on skill overlap, title match and remote preference.\n",
        "    \"\"\"\n",
        "    jd_text = (job.description + \" \" + (job.requirements or \"\")).lower()\n",
        "\n",
        "    skill_hits = sum(1 for s in profile.skills if s.lower() in jd_text)\n",
        "    base_score = skill_hits / max(len(profile.skills), 1)\n",
        "\n",
        "    title_match = any(t.lower() in job.title.lower() for t in prefs.target_titles)\n",
        "    if title_match:\n",
        "        base_score += 0.2\n",
        "\n",
        "    if prefs.remote_only and job.location and \"remote\" not in job.location.lower():\n",
        "        base_score -= 0.3\n",
        "\n",
        "    score = max(0.0, min(1.0, base_score))\n",
        "    selected = score >= 0.3\n",
        "\n",
        "    reason = (\n",
        "        f\"skills_matched={skill_hits}, \"\n",
        "        f\"title_match={title_match}, \"\n",
        "        f\"remote_only={prefs.remote_only}, \"\n",
        "        f\"score={score:.2f}\"\n",
        "    )\n",
        "\n",
        "    log_match(job.job_id, score, selected, reason)\n",
        "    return MatchResult(job=job, score=score, reason=reason, selected=selected)\n",
        "\n",
        "\n",
        "def rank_jobs(profile: UserProfile, prefs: Preferences, jobs: List[JobDescription]) -> List[MatchResult]:\n",
        "    \"\"\"Rank jobs in descending order of match score.\"\"\"\n",
        "    results = [compute_match_score(profile, job, prefs) for job in jobs]\n",
        "    results.sort(key=lambda r: r.score, reverse=True)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "kKYKdBgAxRIe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tailoring and cover letter agents"
      ],
      "metadata": {
        "id": "VQKhhFWJxZ3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Tailoring and cover letters\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def tailor_resume_for_job(profile: UserProfile, job: JobDescription) -> TailoredResult:\n",
        "    \"\"\"\n",
        "    Produce a role-specific summary and a reordered skill list for a job.\n",
        "    Skills must come from the candidate profile.\n",
        "    \"\"\"\n",
        "    system_instruction = (\n",
        "        \"Tailor resume content for a specific job. \"\n",
        "        \"Use only the candidate's existing skills and experience.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Return ONLY valid JSON:\n",
        "\n",
        "{{\n",
        "  \"tailored_summary\": \"2-3 sentences targeting this job.\",\n",
        "  \"ordered_skills\": [\"skill1\", \"skill2\", \"...\"]  // subset + reorder of candidate skills\n",
        "}}\n",
        "\n",
        "JOB:\n",
        "- Title: {job.title}\n",
        "- Company: {job.company}\n",
        "- Description: {job.description}\n",
        "- Requirements: {job.requirements}\n",
        "\n",
        "CANDIDATE_PROFILE:\n",
        "{json.dumps(asdict(profile), indent=2)}\n",
        "\"\"\"\n",
        "\n",
        "    data = call_gemini_json(prompt, system_instruction)\n",
        "\n",
        "    real_skills = set(profile.skills)\n",
        "    ordered = [s for s in data.get(\"ordered_skills\", []) if s in real_skills]\n",
        "    for s in profile.skills:\n",
        "        if s not in ordered:\n",
        "            ordered.append(s)\n",
        "\n",
        "    return TailoredResult(\n",
        "        job_id=job.job_id,\n",
        "        tailored_summary=data.get(\"tailored_summary\", \"\"),\n",
        "        ordered_skills=ordered,\n",
        "    )\n",
        "\n",
        "\n",
        "def generate_cover_letter(profile: UserProfile, job: JobDescription, tailored_summary: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a short cover letter for a specific job using the candidate profile.\n",
        "    \"\"\"\n",
        "    system_instruction = (\n",
        "        \"Write a concise, honest cover letter based only on the candidate profile.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "JOB:\n",
        "- Title: {job.title}\n",
        "- Company: {job.company}\n",
        "- Description: {job.description}\n",
        "\n",
        "CANDIDATE_PROFILE:\n",
        "{json.dumps(asdict(profile), indent=2)}\n",
        "\n",
        "TAILORED_SUMMARY:\n",
        "{tailored_summary}\n",
        "\n",
        "Write the cover letter. Do not introduce skills or employers that are not in the profile.\n",
        "\"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel(MODEL_NAME, system_instruction=system_instruction)\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip()\n"
      ],
      "metadata": {
        "id": "_XMKllIOxRQQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF generation"
      ],
      "metadata": {
        "id": "H-Eo2f0qxagi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# PDF generation\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def build_tailored_resume_pdf(\n",
        "    profile: UserProfile,\n",
        "    tailoring: TailoredResult,\n",
        "    pdf_path: str,\n",
        ") -> None:\n",
        "    \"\"\"Render a compact, ATS-friendly resume PDF.\"\"\"\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.add_page()\n",
        "\n",
        "    # Name\n",
        "    pdf.set_font(\"Helvetica\", \"B\", 16)\n",
        "    pdf.cell(0, 10, sanitize_pdf_text(profile.name), ln=1)\n",
        "\n",
        "    # Summary\n",
        "    pdf.set_font(\"Helvetica\", \"B\", 12)\n",
        "    pdf.cell(0, 8, sanitize_pdf_text(\"Summary\"), ln=1)\n",
        "    pdf.set_font(\"Helvetica\", \"\", 11)\n",
        "    for line in textwrap.wrap(tailoring.tailored_summary, width=90):\n",
        "        pdf.cell(0, 6, sanitize_pdf_text(line), ln=1)\n",
        "    pdf.ln(3)\n",
        "\n",
        "    # Skills\n",
        "    pdf.set_font(\"Helvetica\", \"B\", 12)\n",
        "    pdf.cell(0, 8, sanitize_pdf_text(\"Skills\"), ln=1)\n",
        "    pdf.set_font(\"Helvetica\", \"\", 11)\n",
        "    skills_line = \", \".join(tailoring.ordered_skills)\n",
        "    for line in textwrap.wrap(skills_line, width=90):\n",
        "        pdf.cell(0, 6, sanitize_pdf_text(line), ln=1)\n",
        "    pdf.ln(3)\n",
        "\n",
        "    # Experience\n",
        "    if profile.experience:\n",
        "        pdf.set_font(\"Helvetica\", \"B\", 12)\n",
        "        pdf.cell(0, 8, sanitize_pdf_text(\"Experience\"), ln=1)\n",
        "        pdf.set_font(\"Helvetica\", \"\", 11)\n",
        "        for exp in profile.experience:\n",
        "            title_line = f\"{exp.title} - {exp.company}\"\n",
        "            pdf.cell(0, 6, sanitize_pdf_text(title_line), ln=1)\n",
        "            if exp.start_date or exp.end_date:\n",
        "                date_line = f\"{exp.start_date or ''} - {exp.end_date or 'Present'}\"\n",
        "                pdf.cell(0, 6, sanitize_pdf_text(date_line), ln=1)\n",
        "            for b in exp.bullets:\n",
        "                for line in textwrap.wrap(\"• \" + b, width=90):\n",
        "                    pdf.cell(0, 6, sanitize_pdf_text(line), ln=1)\n",
        "            pdf.ln(2)\n",
        "\n",
        "    # Projects\n",
        "    if profile.projects:\n",
        "        pdf.set_font(\"Helvetica\", \"B\", 12)\n",
        "        pdf.cell(0, 8, sanitize_pdf_text(\"Projects\"), ln=1)\n",
        "        pdf.set_font(\"Helvetica\", \"\", 11)\n",
        "        for proj in profile.projects:\n",
        "            pdf.cell(0, 6, sanitize_pdf_text(proj.name), ln=1)\n",
        "            if proj.description:\n",
        "                for line in textwrap.wrap(proj.description, width=90):\n",
        "                    pdf.cell(0, 6, sanitize_pdf_text(line), ln=1)\n",
        "            for b in proj.bullets:\n",
        "                for line in textwrap.wrap(\"• \" + b, width=90):\n",
        "                    pdf.cell(0, 6, sanitize_pdf_text(line), ln=1)\n",
        "            pdf.ln(2)\n",
        "\n",
        "    # Education\n",
        "    if profile.education:\n",
        "        pdf.set_font(\"Helvetica\", \"B\", 12)\n",
        "        pdf.cell(0, 8, sanitize_pdf_text(\"Education\"), ln=1)\n",
        "        pdf.set_font(\"Helvetica\", \"\", 11)\n",
        "        for edu in profile.education:\n",
        "            for line in textwrap.wrap(edu, width=90):\n",
        "                pdf.cell(0, 6, sanitize_pdf_text(line), ln=1)\n",
        "\n",
        "    pdf.output(pdf_path)\n",
        "\n",
        "\n",
        "def build_cover_letter_pdf(\n",
        "    cover_letter_text: str,\n",
        "    profile: UserProfile,\n",
        "    job: JobDescription,\n",
        "    pdf_path: str,\n",
        ") -> None:\n",
        "    \"\"\"Render a cover letter PDF from plain text.\"\"\"\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.add_page()\n",
        "\n",
        "    # Name\n",
        "    pdf.set_font(\"Helvetica\", \"B\", 14)\n",
        "    pdf.cell(0, 10, sanitize_pdf_text(profile.name), ln=1)\n",
        "\n",
        "    pdf.set_font(\"Helvetica\", \"\", 11)\n",
        "    pdf.ln(4)\n",
        "\n",
        "    # Header line\n",
        "    header = f\"Cover Letter - {job.title} at {job.company}\"\n",
        "    for line in textwrap.wrap(header, width=90):\n",
        "        pdf.cell(0, 6, sanitize_pdf_text(line), ln=1)\n",
        "\n",
        "    pdf.ln(4)\n",
        "    for line in cover_letter_text.splitlines():\n",
        "        if not line.strip():\n",
        "            pdf.ln(3)\n",
        "            continue\n",
        "        for wrapped in textwrap.wrap(line, width=90):\n",
        "            pdf.cell(0, 6, sanitize_pdf_text(wrapped), ln=1)\n",
        "\n",
        "    pdf.output(pdf_path)\n"
      ],
      "metadata": {
        "id": "0IVwNP3FQxBF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Orchestrator"
      ],
      "metadata": {
        "id": "bK8SrmPrxa_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Orchestrator\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def run_intelliapply_pipeline(\n",
        "    resume_text: str,\n",
        "    preferences: Preferences,\n",
        "    jobs: List[JobDescription],\n",
        "    top_k: int = 5,\n",
        ") -> List[ApplicationPackage]:\n",
        "    \"\"\"\n",
        "    Execute the end-to-end flow:\n",
        "    - build profile\n",
        "    - rank jobs\n",
        "    - for top_k jobs above a threshold: tailor content, generate cover letter\n",
        "    - capture user decision and export PDFs when requested\n",
        "    \"\"\"\n",
        "    profile = setup_profile_and_prefs(resume_text, preferences)\n",
        "    ranked = rank_jobs(profile, preferences, jobs)\n",
        "    candidates = [r for r in ranked if r.selected][:top_k]\n",
        "\n",
        "    packages: List[ApplicationPackage] = []\n",
        "\n",
        "    for match in candidates:\n",
        "        job = match.job\n",
        "\n",
        "        if any(h[\"job_id\"] == job.job_id for h in APPLIED_JOBS_HISTORY):\n",
        "            logger.info(\"job %s already handled in this session\", job.job_id)\n",
        "            continue\n",
        "\n",
        "        tailoring = tailor_resume_for_job(profile, job)\n",
        "        cover_letter = generate_cover_letter(profile, job, tailoring.tailored_summary)\n",
        "\n",
        "        pkg = ApplicationPackage(\n",
        "            job=job,\n",
        "            match_score=match.score,\n",
        "            match_reason=match.reason,\n",
        "            tailored_summary=tailoring.tailored_summary,\n",
        "            ordered_skills=tailoring.ordered_skills,\n",
        "            cover_letter=cover_letter,\n",
        "        )\n",
        "        packages.append(pkg)\n",
        "\n",
        "        print(f\"\\n=== {job.title} @ {job.company} ===\")\n",
        "        print(f\"Location: {job.location}\")\n",
        "        print(f\"Match score: {match.score:.2f}\")\n",
        "        print(\"Reason:\", match.reason)\n",
        "        print(\"\\nTailored summary:\\n\", tailoring.tailored_summary)\n",
        "        print(\"\\nOrdered skills:\", tailoring.ordered_skills)\n",
        "        print(\"\\nCover letter:\\n\", cover_letter)\n",
        "\n",
        "        decision = input(\"\\nApply to this job? (y/n/save): \").strip().lower()\n",
        "        if decision not in (\"y\", \"n\", \"save\"):\n",
        "            decision = \"n\"\n",
        "\n",
        "        remember_application(job, decision, match.score)\n",
        "        print(\"Decision recorded:\", decision)\n",
        "\n",
        "        if decision in (\"y\", \"save\"):\n",
        "            safe_title = \"\".join(\n",
        "                c for c in job.title if c.isalnum() or c in (\" \", \"_\")\n",
        "            ).strip().replace(\" \", \"_\")\n",
        "            base_name = f\"{safe_title}_{job.job_id}\"\n",
        "            resume_pdf = f\"{base_name}_resume.pdf\"\n",
        "            cover_pdf = f\"{base_name}_cover_letter.pdf\"\n",
        "\n",
        "            build_tailored_resume_pdf(profile, tailoring, resume_pdf)\n",
        "            build_cover_letter_pdf(cover_letter, profile, job, cover_pdf)\n",
        "\n",
        "            print(\"Generated files:\")\n",
        "            print(\"  \", resume_pdf)\n",
        "            print(\"  \", cover_pdf)\n",
        "\n",
        "    return packages\n"
      ],
      "metadata": {
        "id": "1vDnrJvaxRff"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main execution cell"
      ],
      "metadata": {
        "id": "ojieFspbxbeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Main execution\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "resume_text = load_resume_text_from_upload()\n",
        "print(\"\\nExcerpt from extracted resume text:\\n\")\n",
        "print(resume_text[:500])\n",
        "\n",
        "preferences = extract_preferences_from_resume_text(resume_text)\n",
        "print(\"\\nInferred preferences:\\n\", preferences)\n",
        "\n",
        "jobs = fetch_jobs_for_preferences(preferences, per_title=10)\n",
        "print(f\"\\nFetched {len(jobs)} jobs for titles {preferences.target_titles}\")\n",
        "for j in jobs[:5]:\n",
        "    print(f\"- {j.title} @ {j.company} ({j.location})\")\n",
        "\n",
        "packages = run_intelliapply_pipeline(\n",
        "    resume_text=resume_text,\n",
        "    preferences=preferences,\n",
        "    jobs=jobs,\n",
        "    top_k=5,\n",
        ")\n",
        "\n",
        "print(\"\\nApplication history:\")\n",
        "for entry in APPLIED_JOBS_HISTORY:\n",
        "    print(entry)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pDclcP7zxRnX",
        "outputId": "15c095da-3745-4dd4-a819-ce74da45679f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your resume file (PDF, DOCX, or TXT).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c4732657-d555-4bc6-beb4-d28f0313b359\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c4732657-d555-4bc6-beb4-d28f0313b359\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test_resume.txt to test_resume (2).txt\n",
            "Loaded file: test_resume (2).txt\n",
            "\n",
            "Excerpt from extracted resume text:\n",
            "\n",
            "John Doe  \r\n",
            "(555) 987-6543 | john.doe.data@testmail.com  \r\n",
            "LinkedIn: linkedin.com/in/johndoe-data  \r\n",
            "GitHub: github.com/johndoe  \r\n",
            "\r\n",
            "PROFESSIONAL SUMMARY  \r\n",
            "Results-driven Data Engineer with 4+ years of experience building scalable data pipelines, cloud data solutions, and analytics platforms. Strong background in distributed computing, ETL orchestration, and enterprise data modeling. Skilled in Python, SQL, Spark, Azure, AWS, and Databricks. Experienced in delivering end-to-end data systems acr\n",
            "\n",
            "Inferred preferences:\n",
            " Preferences(target_titles=['Data Engineer', 'Data Scientist', 'Analytics Engineer'], locations=['United States', 'Remote'], remote_only=False, min_salary=None, preferred_companies=[])\n",
            "\n",
            "Fetched 4 jobs for titles ['Data Engineer', 'Data Scientist', 'Analytics Engineer']\n",
            "- Tech Lead Databricks Data Engineer @ Mitre Media (USA, Canada, USA timezones)\n",
            "- Senior Data Engineer (AWS & Python) @ Proxify (CET +/- 3 HOURS)\n",
            "- Senior Data Engineer (Azure) @ Proxify (CET +/- 3 HOURS)\n",
            "- Quantitative Research Team Lead (Completed) @ Apexver (Worldwide)\n",
            "\n",
            "=== Tech Lead Databricks Data Engineer @ Mitre Media ===\n",
            "Location: USA, Canada, USA timezones\n",
            "Match score: 0.40\n",
            "Reason: skills_matched=9, title_match=True, remote_only=False, score=0.40\n",
            "\n",
            "Tailored summary:\n",
            " Highly accomplished Data Engineer with 4+ years of experience, specializing in architecting and optimizing large-scale Databricks-based ETL pipelines and cloud data platforms. Proven expertise in leveraging Spark, Python, and SQL to transform complex data into actionable insights, enhance data quality, and enable AI/ML workflows for user-facing features. Adept at evolving cloud infrastructure (AWS/Azure) for performance and cost-efficiency, with a passion for FinTech innovation and mentoring teams to deliver robust data solutions.\n",
            "\n",
            "Ordered skills: ['Python', 'SQL', 'DBT', 'Java', 'Tableau', 'Power BI', 'Docker', 'JavaScript', 'R', 'Bash', 'Apache Spark', 'PySpark', 'SparkSQL', 'Airflow', 'Data Factory', 'Fivetran', 'PostgreSQL', 'MySQL', 'Oracle', 'MongoDB', 'DynamoDB', 'AWS Lambda', 'AWS S3', 'AWS Glue', 'AWS Redshift', 'AWS EMR', 'AWS CloudWatch', 'AWS IAM', 'Azure Data Lake', 'Azure Databricks', 'Azure Functions', 'Azure Cosmos DB', 'Kubernetes', 'GitHub Actions', 'Jenkins', 'QuickSight', 'Plotly', 'Seaborn', 'Flask', 'Spring Boot', 'React.js', 'Jira', 'Confluence', 'Asana', 'Git']\n",
            "\n",
            "Cover letter:\n",
            " Dear Mitre Media Hiring Team,\n",
            "\n",
            "I am writing to express my enthusiastic interest in the Tech Lead Databricks Data Engineer position at Mitre Media. With over 4 years of experience specializing in architecting and optimizing large-scale Databricks-based ETL pipelines, coupled with a passion for AI and FinTech innovation, I am eager to contribute to your mission of redefining FinTech.\n",
            "\n",
            "My background includes extensive experience designing, implementing, and optimizing large-scale ETL workflows using Databricks, Apache Spark, Python, and SQL. I have developed automated ETL pipelines on Azure Databricks and AWS Glue, significantly improving data refresh latency and pipeline reliability. My expertise also spans evolving cloud data platforms across AWS and Azure for performance and cost-efficiency, alongside instituting robust data validation frameworks and monitoring solutions, including Azure Monitor and AWS CloudWatch, to ensure data quality and reduce production failures.\n",
            "\n",
            "I am adept at transforming complex data into actionable insights, having built PySpark data models and developed SparkSQL transformations. My experience further includes enabling AI/ML workflows by building inference-ready tables, and my project on CryptoTrend Prediction directly showcases my interest in financial market data. As a self-starter with a mentorship attitude, I thrive in distributed, cross-functional teams, collaborating with stakeholders using Agile workflows and documenting data workflows to uplift peers.\n",
            "\n",
            "I am particularly excited by the opportunity to architect the data backbone powering every feature across your product suite and to contribute to cutting-edge AI-driven solutions for millions of investors. My skills in data engineering programming, Databricks, Spark, SQL, DBT, and cloud data services align strongly with your requirements.\n",
            "\n",
            "Thank you for your time and consideration. I have attached my resume and GitHub profile for your review and welcome the opportunity to discuss how my experience can benefit Mitre Media.\n",
            "\n",
            "Sincerely,\n",
            "John Doe\n",
            "\n",
            "Apply to this job? (y/n/save): y\n",
            "Decision recorded: y\n",
            "Generated files:\n",
            "   Tech_Lead_Databricks_Data_Engineer_2069747_resume.pdf\n",
            "   Tech_Lead_Databricks_Data_Engineer_2069747_cover_letter.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4113999123.py:17: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 10, sanitize_pdf_text(profile.name), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:21: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 8, sanitize_pdf_text(\"Summary\"), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:24: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 6, sanitize_pdf_text(line), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:29: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 8, sanitize_pdf_text(\"Skills\"), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:33: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 6, sanitize_pdf_text(line), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:39: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 8, sanitize_pdf_text(\"Experience\"), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:43: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 6, sanitize_pdf_text(title_line), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:46: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 6, sanitize_pdf_text(date_line), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:49: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 6, sanitize_pdf_text(line), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:55: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 8, sanitize_pdf_text(\"Projects\"), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:58: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 6, sanitize_pdf_text(proj.name), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:61: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 6, sanitize_pdf_text(line), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:64: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 6, sanitize_pdf_text(line), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:70: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 8, sanitize_pdf_text(\"Education\"), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:74: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 6, sanitize_pdf_text(line), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:92: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 10, sanitize_pdf_text(profile.name), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:100: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 6, sanitize_pdf_text(line), ln=1)\n",
            "/tmp/ipython-input-4113999123.py:108: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 6, sanitize_pdf_text(wrapped), ln=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Senior Data Engineer (AWS & Python) @ Proxify ===\n",
            "Location: CET +/- 3 HOURS\n",
            "Match score: 0.36\n",
            "Reason: skills_matched=7, title_match=True, remote_only=False, score=0.36\n",
            "\n",
            "Tailored summary:\n",
            " Results-driven Data Engineer with 4+ years of professional experience specializing in modern, cloud-native data platforms, with a strong focus on Amazon Web Services (AWS) and Python. Possessing deep expertise in designing, building, and optimizing highly scalable ETL/ELT pipelines and data warehouses using AWS S3, Glue, EMR, Redshift, and PySpark to power analytics and business intelligence. Adept at advanced SQL for complex query writing and optimization, coupled with a solid understanding of containerization and orchestration using Docker and Kubernetes.\n",
            "\n",
            "Ordered skills: ['Python', 'AWS S3', 'AWS Glue', 'AWS EMR', 'AWS Redshift', 'SQL', 'PySpark', 'Apache Spark', 'Airflow', 'Docker', 'Kubernetes', 'AWS Lambda', 'AWS CloudWatch', 'SparkSQL', 'PostgreSQL', 'MySQL', 'DBT', 'Azure Databricks', 'GitHub Actions', 'Git', 'Java', 'JavaScript', 'R', 'Bash', 'Data Factory', 'Fivetran', 'Oracle', 'MongoDB', 'DynamoDB', 'AWS IAM', 'Azure Data Lake', 'Azure Functions', 'Azure Cosmos DB', 'Jenkins', 'Power BI', 'Tableau', 'QuickSight', 'Plotly', 'Seaborn', 'Flask', 'Spring Boot', 'React.js', 'Jira', 'Confluence', 'Asana']\n",
            "\n",
            "Cover letter:\n",
            " Subject: Application for Senior Data Engineer - John Doe\n",
            "\n",
            "Dear Proxify Hiring Team,\n",
            "\n",
            "I am writing to express my strong interest in the Senior Data Engineer position at Proxify. With over 4 years of professional experience, I specialize in modern, cloud-native data platforms, with a strong focus on Amazon Web Services (AWS) and Python, directly aligning with your requirements.\n",
            "\n",
            "My background includes extensive experience in designing, building, and optimizing highly scalable ETL/ELT pipelines and data warehouses. I possess deep hands-on expertise with core AWS services such as S3, Glue, Lambda, EMR, and Redshift, having developed automated ETL pipelines and serverless log-processing stacks. I am proficient in Python for data manipulation and pipeline development, including PySpark, and have solid expertise in SQL for complex query writing and optimization, designing robust data models and optimizing Spark transformations across various datasets. Furthermore, my skills extend to containerization and orchestration concepts with Docker and Kubernetes, and I have experience with orchestration tools like Apache Airflow.\n",
            "\n",
            "I am adept at architecting and maintaining scalable data pipelines using Python and native AWS services, and have experience serving as a subject matter expert for core AWS data services. My experience includes designing robust and efficient data models for analytical needs, performing performance tuning and query optimization, and integrating data pipelines into modern CI/CD processes for automated deployment. I have also implemented data validation frameworks and automated log monitoring with CloudWatch, demonstrating a commitment to data quality and governance.\n",
            "\n",
            "I am eager to bring my expertise in AWS, Python, and scalable data solutions to Proxify and contribute to your clients' success. Thank you for considering my application. I look forward to discussing how my skills and experience can benefit your team.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "John Doe\n",
            "\n",
            "Apply to this job? (y/n/save): y\n",
            "Decision recorded: y\n",
            "Generated files:\n",
            "   Senior_Data_Engineer_AWS__Python_1591692_resume.pdf\n",
            "   Senior_Data_Engineer_AWS__Python_1591692_cover_letter.pdf\n",
            "\n",
            "=== Senior Data Engineer (Azure) @ Proxify ===\n",
            "Location: CET +/- 3 HOURS\n",
            "Match score: 0.33\n",
            "Reason: skills_matched=6, title_match=True, remote_only=False, score=0.33\n",
            "\n",
            "Tailored summary:\n",
            " A results-driven Data Engineer with over 4 years of experience, specializing in Microsoft Azure technologies, eager to contribute to Proxify's fast-growing Network. I possess deep expertise in designing and optimizing end-to-end ETL pipelines, complex data models, and data warehouses using Azure Data Factory, Databricks, and advanced SQL. My background includes proven success in data integration, governance, and Power BI development, aligning perfectly with the responsibilities of a Senior Data Engineer.\n",
            "\n",
            "Ordered skills: ['Azure Databricks', 'Azure Data Lake', 'Azure Functions', 'Azure Cosmos DB', 'SQL', 'PostgreSQL', 'MySQL', 'Apache Spark', 'PySpark', 'SparkSQL', 'Airflow', 'DBT', 'Power BI', 'Python', 'Git', 'GitHub Actions', 'Docker', 'Jira', 'Confluence', 'Asana', 'Java', 'JavaScript', 'R', 'Bash', 'Data Factory', 'Fivetran', 'Oracle', 'MongoDB', 'DynamoDB', 'AWS Lambda', 'AWS S3', 'AWS Glue', 'AWS Redshift', 'AWS EMR', 'AWS CloudWatch', 'AWS IAM', 'Kubernetes', 'Jenkins', 'Tableau', 'QuickSight', 'Plotly', 'Seaborn', 'Flask', 'Spring Boot', 'React.js']\n",
            "\n",
            "Cover letter:\n",
            " Dear Proxify Hiring Team,\n",
            "\n",
            "I am writing to express my enthusiastic interest in the Senior Data Engineer (Azure) position at Proxify. As a results-driven Data Engineer with over 4 years of experience, specializing in Microsoft Azure technologies, I am eager to contribute to your fast-growing Network. I possess deep expertise in designing and optimizing end-to-end ETL pipelines, complex data models, and data warehouses using Azure Data Factory, Databricks, and advanced SQL. My background includes proven success in data integration, governance, and Power BI development, aligning perfectly with the responsibilities of a Senior Data Engineer.\n",
            "\n",
            "Throughout my experience, particularly in my roles at QuantumCare Health Systems, I have developed automated ETL pipelines using Azure Data Factory and Azure Databricks, significantly improving data refresh latency and creating Azure Monitor alerting pipelines. I have designed PySpark data models, re-engineered legacy ETL jobs with PySpark and Airflow DAGs, and optimized Spark transformations, demonstrating strong capabilities in pipeline orchestration and data processing. My advanced SQL knowledge has been crucial for designing data-quality scripts and supporting robust data solutions.\n",
            "\n",
            "I have a deep understanding of data modeling principles, having designed PySpark data models and implemented complex SQL data-quality scripts validating medical datasets. My experience includes developing and supporting Power BI dashboards, where I've supported clinical throughput monitoring and built executive dashboards tracking key performance indicators. I am adept at implementing data validation frameworks to ensure data governance and anomaly detection, crucial for maintaining data quality and integrity across the BI ecosystem. My skills in Python and Git further support robust development practices.\n",
            "\n",
            "My proven track record in integrating data from various sources, optimizing ETL workflows, and collaborating with stakeholders to translate business requirements into technical solutions makes me a strong candidate. I am confident I possess the skills and dedication to excel in this role at Proxify and contribute to developing exciting products.\n",
            "\n",
            "Thank you for your time and consideration.\n",
            "\n",
            "Sincerely,\n",
            "John Doe\n",
            "\n",
            "Apply to this job? (y/n/save): n\n",
            "Decision recorded: n\n",
            "\n",
            "Application history:\n",
            "{'job_id': '2069747', 'title': 'Tech Lead Databricks Data Engineer', 'company': 'Mitre Media', 'decision': 'y', 'score': 0.4, 'timestamp_utc': '2025-12-01T19:08:21.575018+00:00'}\n",
            "{'job_id': '1591692', 'title': 'Senior Data Engineer (AWS & Python)', 'company': 'Proxify', 'decision': 'y', 'score': 0.35555555555555557, 'timestamp_utc': '2025-12-01T19:08:59.761731+00:00'}\n",
            "{'job_id': '1383531', 'title': 'Senior Data Engineer (Azure)', 'company': 'Proxify', 'decision': 'n', 'score': 0.33333333333333337, 'timestamp_utc': '2025-12-01T19:09:29.052153+00:00'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Simple evaluation: agent score vs human labels\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "eval_jobs = [\n",
        "    JobDescription(\n",
        "        job_id=\"e1\",\n",
        "        title=\"ML Intern\",\n",
        "        company=\"EvalCo\",\n",
        "        location=\"Remote\",\n",
        "        salary_estimate=None,\n",
        "        description=\"Data or ML intern with Python and Power BI experience.\",\n",
        "        requirements=\"Python, deep learning, Power BI\",\n",
        "    ),\n",
        "    JobDescription(\n",
        "        job_id=\"e2\",\n",
        "        title=\"Sales Intern\",\n",
        "        company=\"NonTechCo\",\n",
        "        location=\"Onsite\",\n",
        "        salary_estimate=None,\n",
        "        description=\"Sales role with no programming or data responsibilities.\",\n",
        "        requirements=\"communication, sales\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Ground-truth labels for evaluation\n",
        "human_labels = {\n",
        "    \"e1\": \"good_fit\",\n",
        "    \"e2\": \"poor_fit\",\n",
        "}\n",
        "\n",
        "# Use the existing profile and preferences from the main run\n",
        "profile_for_eval = USER_PROFILE_STORE or parse_resume_to_profile(resume_text)\n",
        "results_eval = rank_jobs(profile_for_eval, preferences, eval_jobs)\n",
        "\n",
        "print(\"\\nEvaluation (agent score vs human label):\")\n",
        "for r in results_eval:\n",
        "    print(\n",
        "        f\"Job {r.job.job_id} ({r.job.title}): \"\n",
        "        f\"score={r.score:.2f}, label={human_labels[r.job.job_id]}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj_wJP5j7aeE",
        "outputId": "88f7a4cc-a19d-42b7-9d03-ac292fe4001e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation (agent score vs human label):\n",
            "Job e1 (ML Intern): score=0.07, label=good_fit\n",
            "Job e2 (Sales Intern): score=0.02, label=poor_fit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zZ4H9n6Pwmrd"
      }
    }
  ]
}